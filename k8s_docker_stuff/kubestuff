# Creating Objects
======================================================================================================
## IMPERATIVE Paradigm - no embedded history
# kubectl run - creates deployment or jobs
# kubectl expose - creates services
# kubectl set,edit,patch - top dog
# kubectl replace -f deployment.yaml --save-config

# Creates a deployment + pod
kubectl create deployment nginx --image=nginx

# Create a pod (the minimum needed flag is --image)
kubectl run nginx --image=nginx
# Create a pod named basic with port 80 open to TCP
kubectl run -n red --image=nginx:stable-alpine-perl --restart=OnFailure --port=80 basic
# Create a pod using  dry-run, e.g. the --image option is required
kubectl -run --image=nginx:stable-alpine-perl basic --dry-run=client -o yaml

# discover the IP of all pods in sec1 namespace
kubectl -n sec1 get po -ojsonpath={.items[*].status.podIP}

# discover the IP of po named basic
kubectl get pods basic -n sec1 -o jsonpath='{.status.podIP}'

# ping from 
kubectl -n sec1 exec -it pod2 -- ping $pod1IP

# Creating SERVICEs
======================================================================================================

# creates a local service to access a ClusterIP, usefull for troubleshooting and provides quick way to check your service
kubectl proxy

# create service
kubectl -n red expose po basic --name=cloudacademy-svc --type="ClusterIP" --port=8080 --target-port=80

# Expose a Pod in the red Namespace with the following configuration: The Service name is cloudacademy, the Service port is8080, the Target port is80, the Service type isClusterIP
kubectl expose pod basic -n red --name=cloudacademy-svc --port=8080 --target-port=80

# create service NodePort on port 32080: expose deployment for an app that runs on port 80 Service 
# The command will assign a random port >= 30000. So use the Patch command to assign the port to a known, unused and desired port >= 30000

kubectl -n ca1 expose deployment cloudforce --name=cloudforce-svc --type="NodePort" --port=80
kubectl -n ca1 patch service cloudforce-svc --type='json' --patch='[{"op": "replace", "path": "/spec/ports/0/nodePort", "value":32080}]'

# all the above in one command
kubectl -n ca1 expose --type=NodePort deployment cloudforce --port 80 --name cloudforce-svc --overrides '{ "apiVersion": "v1","spec":{"ports": [{"port":80,"protocol":"TCP","targetPort":
80,"nodePort":32080}]}}'

# create a nodeport service
kubectl -n accounting expose deployment nginx-one --type=NodePort --name=service-lab

# spin up pod that run curl to a service
kubectl run client -n skynet --image=appropriate/curl -it --rm --restart=Never -- curl http://t2-svc:8080 > /home/ubuntu/svc-output.txt


## Upgrade K8s cluster (worker/cp)
======================================================================================================

mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config

# check kubelet status
systemctl status kubelet.service

# drain worker node, draining will cordon the node also (do not forget to uncordon the node)
kubectl drain --ignore-daemonsets <node>

# ssh into the worker node
# Update the package index
sudo apt-get update

# find the latest patch release version for Kubernetes using OS package manager e.g:  1.26.3-00
# the same as apt list -a <package name>
apt-cache madison kubeadm


# cancel hold on packages
sudo apt-mark unhold kubelet kubeadm kubectl

# Install the packages
sudo apt-get install -y kubeadm=1.26.3-00 kubelet=1.26.3-00 kubectl=1.26.3-00

# prevent automatic updates to the following packages
sudo apt-mark hold kubelet kubectl kubeadm


# For worker nodes this upgrades the local kubelet configuration: /var/lib/kubelet/config.yaml
# update kubelet configuration for worker node
sudo kubeadm upgrade node

sudo systemctl daemon-reload
sudo systemctl restart kubelet

# All containers are restarted after upgrade, because the container spec hash value is changed.


# Update packages required for HTTPS package repository access
sudo apt-get install -y apt-transport-https ca-certificates curl software-properties-common gnupg lsb-release


# Add the Google Cloud packages GPG key
curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -
# Add the Kubernetes release repository
sudo add-apt-repository "deb http://apt.kubernetes.io/ kubernetes-xenial main"

# Update the package index to include the Kubernetes repository
sudo apt-get update

# create control plane node
kubeadm init

# create worker node
kubeadm join

## Deployments
======================================================================================================

# create deployment webserver yaml file based on ngnix image
kubectl create deploy webserver --image nginx:1.22.1 --replicas=2 --dry-run=client -o yaml | tee dep.yaml

# get the deployment strategy
kubectl get deploy <deployment> -o yaml | grep -A 4 strategy

# change the base image for the deployment
kubectl set image deploy webserver nginx=nginx:1.23.1-alpine

# Deployment strategies / Update strategies: RollingUpdate vs. Recreate 
# return to a previous version
kubectl rollout undo

# view object revisions
kubectl rollout history deploy <deployment>

# create deployment named rtro
kubectl -n cal create deployment rtro --image=busybox:1.31.1
kubectl -n cal edit deploymnet rtro
 spec:
      containers:
      - args:
        - sleep
        - 24h
        image: busybox:1.31.1
        imagePullPolicy: IfNotPresent
        name: busybox
        resources: {}
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
      dnsPolicy: ClusterFirst
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext:
        runAsGroup: 10000
        runAsUser: 1000
      terminationGracePeriodSeconds: 30


## Secrets
======================================================================================================
# Secret = key/value pairs of sensitive data that can be accessed by pods (encoded in base64) so describe will show opaque data 

# create secret
kubectl create secret generic mysql --from-literal=password=root

# describe secret
kubectl -n istio-system describe secret ingress-cert-cacert

# take the ca.crt from secret
kubectl get secrets -n istio-system ingress-cert-cacert -o json | jq -r '.data."ca.crt"' | base64 -d > ca.crt

# delete secret
kubectl -n istio-system delete secret ingress-cert-cacert

# create ingress-cert-cacert
kubectl -n istio-system create secret generic ingress-cert-cacert --from-file=ca.crt --from-file=Publiccert.cer

# SECRETS example scenario
kubectl create secret generic <secret_name> --from-literal=MYSQL_ROOT_PASSWORD=test
kubectl get secrets <secret_name> -o yaml > sqlsecret.yml

# configmap can be created from literal values, or individual files


## Scheduling 
======================================================================================================
# check node labels
kubectl describe nodes|grep -A5 -i label

# check node taints
kubectl describe nodes|grep -A5 -i taint

# check how many containers are running on a specific node, ssh into the node and run
sudo crictl ps | wc -lsudo crictl ps | wc -l

# check the maximum number of pods supported by a node
kubectl describe no <node> |  grep -i capacity -A10

# check running pods on a specific node
kubectl describe node <node> | grep -A10 "Non-terminated Pods"

# label node
kubectl label node <node> <label>=<label_values>

# view nodes labels
kubect get nodes --show-labels


## Get container/images from pod
======================================================================================================

# look at the specs
kubectl explain pod.spec.containers

# get container running in a SINGLE POD
kubectl -n <namespace> get pod <pod_name>  -o jsonpath="{.spec.containers[*].name}"

# get container running in a NAMESPACE
kubectl -n <namespace>  get pod -o jsonpath="{.items[*].spec.containers[*].name}"


## Storage
======================================================================================================

# get persistent volumes aka storage abstraction
kubectl get pv

# get  persistent volume claims 
kubectl get pvc